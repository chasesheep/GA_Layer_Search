"""
çœŸå®MMLUè¯„ä¼°é€‚åº”åº¦å‡½æ•° (åŸºäºadaptive beam searchçš„æˆåŠŸç»éªŒ)
"""
import os
import sys
import torch
import time
import warnings
import logging
from pathlib import Path
from typing import List, Dict, Optional, Callable
import io

# æ·»åŠ Gather-and-Aggregateåˆ°è·¯å¾„
GA_DIR = str(Path(__file__).parent.parent / "Gather-and-Aggregate")
if GA_DIR not in sys.path:
    sys.path.insert(0, GA_DIR)

# Mute all warnings and logging (å’Œbeam searchä¸€æ ·)
warnings.filterwarnings("ignore")
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"
os.environ["HF_HUB_DISABLE_TELEMETRY"] = "1"
os.environ["TRANSFORMERS_VERBOSITY"] = "error"
os.environ["DATASETS_VERBOSITY"] = "error"
os.environ["PYTHONWARNINGS"] = "ignore"

# Set all loggers to CRITICAL
logging.getLogger().setLevel(logging.CRITICAL)
for logger_name in ["transformers", "datasets", "lm_eval", "modelscope", 
                     "huggingface_hub", "urllib3", "requests", "torch", "torch.cuda"]:
    logging.getLogger(logger_name).setLevel(logging.CRITICAL)

# Suppress warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)


def load_extracted_layer(layers_dir, layer_idx, device='cuda'):
    """åŠ è½½é¢„æå–çš„å±‚æ–‡ä»¶ï¼ˆå’Œbeam searchä¸€æ ·ï¼‰"""
    layers_path = Path(layers_dir)
    layer_path = layers_path / f"layer_{layer_idx:02d}.pt"
    
    if not layer_path.exists():
        raise FileNotFoundError(f"Layer {layer_idx} not found at {layer_path}")
    
    layer = torch.load(layer_path, map_location=device, weights_only=False)
    return layer


def load_extracted_rotary_emb(layers_dir, device='cuda'):
    """åŠ è½½é¢„æå–çš„rotary_embï¼ˆå’Œbeam searchä¸€æ ·ï¼‰"""
    layers_path = Path(layers_dir)
    rotary_emb_path = layers_path / "rotary_emb.pt"
    
    if not rotary_emb_path.exists():
        raise FileNotFoundError(f"Rotary embeddings not found at {rotary_emb_path}")
    
    rotary_emb = torch.load(rotary_emb_path, map_location=device, weights_only=False)
    return rotary_emb


def eval_mmlu_with_replacement(model, tokenizer, replaced_layers, llama_layers_dir, 
                              limit=None, batch_size=16, use_cache=True, verbose=False):
    """
    ä½¿ç”¨æŒ‡å®šå±‚æ›¿æ¢è¿›è¡ŒMMLUè¯„ä¼°ï¼ˆå’Œbeam searchå®Œå…¨ä¸€æ ·ï¼‰
    
    Args:
        model: Llambaæ¨¡å‹
        tokenizer: tokenizer
        replaced_layers: è¦æ›¿æ¢çš„å±‚ç´¢å¼•åˆ—è¡¨
        llama_layers_dir: Llamaå±‚ç›®å½•
        limit: è¯„ä¼°æ ·æœ¬é™åˆ¶
        batch_size: æ‰¹æ¬¡å¤§å°
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    limit_str = "full" if limit is None else str(limit)
    if verbose:
        print(f"    â³ MMLU evaluation with layers {replaced_layers} (limit={limit_str})...")
    
    try:
        from modelscope_utils import run_eval
        
        # ä¿å­˜åŸå§‹çŠ¶æ€
        original_layers = {}
        original_rotary_emb = None
        if hasattr(model.backbone, 'rotary_emb'):
            original_rotary_emb = model.backbone.rotary_emb
        
        # åŠ è½½å¹¶æ›¿æ¢æŒ‡å®šå±‚
        llama_rotary_emb = load_extracted_rotary_emb(llama_layers_dir, device=model.device)
        model.backbone.rotary_emb = llama_rotary_emb
        
        # å­˜å‚¨åŠ è½½çš„Llamaå±‚ï¼Œç”¨äºåç»­æ¸…ç†
        loaded_llama_layers = []
        for layer_idx in replaced_layers:
            original_layers[layer_idx] = model.backbone.layers[layer_idx]
            llama_layer = load_extracted_layer(llama_layers_dir, layer_idx, device=model.device)
            model.backbone.layers[layer_idx] = llama_layer
            loaded_llama_layers.append(llama_layer)
        
        # ç«‹å³æ¸…ç†åŠ è½½çš„Llamaå±‚ï¼Œé¿å…ç´¯ç§¯æ˜¾å­˜å ç”¨
        del llama_rotary_emb
        for llama_layer in loaded_llama_layers:
            del llama_layer
        torch.cuda.empty_cache()
        import gc
        gc.collect()
        
        start_time = time.time()
        
        # è¿è¡Œè¯„ä¼°ï¼Œé‡å®šå‘stderrä»¥æŠ‘åˆ¶warning
        old_stderr = sys.stderr
        sys.stderr = io.StringIO()
        
        try:
            results = run_eval(
                model=model, 
                tokenizer=tokenizer, 
                tasks=["mmlu"],
                limit=limit,
                batch_size=batch_size,
                cache_requests=use_cache
            )
        finally:
            # æ¢å¤åŸå§‹stderr
            sys.stderr = old_stderr
        
        # è¯„ä¼°å®Œæˆåç«‹å³æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()
        gc.collect()
        torch.cuda.synchronize()
        eval_time = time.time() - start_time
        
        # æ¢å¤åŸå§‹çŠ¶æ€
        for layer_idx in replaced_layers:
            model.backbone.layers[layer_idx] = original_layers[layer_idx]
        if original_rotary_emb is not None:
            model.backbone.rotary_emb = original_rotary_emb
        
        if 'results' in results and 'mmlu' in results['results']:
            mmlu_score = results['results']['mmlu']['acc,none']
            if verbose:
                print(f"    âœ… MMLU completed in {eval_time:.2f}s: {mmlu_score:.4f}")
            
            return {
                'replaced_layers': replaced_layers,
                'score': mmlu_score,
                'time': eval_time,
                'success': True
            }
        else:
            if verbose:
                print(f"    âŒ No MMLU results found")
            return {
                'replaced_layers': replaced_layers,
                'score': 0.0,
                'time': 0.0,
                'success': False,
                'error': "No MMLU results found"
            }
        
    except Exception as e:
        if verbose:
            print(f"    âŒ MMLU evaluation failed: {e}")
        return {
            'replaced_layers': replaced_layers,
            'score': 0.0,
            'time': 0.0,
            'success': False,
            'error': str(e)
        }


class MMLUFitnessFunction:
    """
    MMLUé€‚åº”åº¦å‡½æ•°åŒ…è£…å™¨
    
    ä½¿ç”¨å•ä¾‹æ¨¡å¼ï¼Œç¡®ä¿æ¨¡å‹åªåŠ è½½ä¸€æ¬¡
    """
    _instance = None
    _model = None
    _tokenizer = None
    _llama_layers_dir = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    @classmethod
    def initialize(cls, llama_layers_dir: str, gpu_id: int = None, verbose: bool = True):
        """
        åˆå§‹åŒ–æ¨¡å‹ï¼ˆåªè°ƒç”¨ä¸€æ¬¡ï¼‰
        
        Args:
            llama_layers_dir: Llamaå±‚ç›®å½•
            gpu_id: GPU ID
            verbose: æ˜¯å¦æ‰“å°ä¿¡æ¯
        """
        if cls._model is not None:
            if verbose:
                print("  â„¹ï¸  Model already loaded, skipping initialization")
            return
        
        # è®¾ç½®GPU
        if gpu_id is not None:
            os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
            if verbose:
                print(f"  ğŸ® Using GPU: {gpu_id}")
        
        cls._llama_layers_dir = llama_layers_dir
        
        if verbose:
            print("\nğŸ§ª Loading Llamba model...")
        
        try:
            from modelscope_utils import get_model_modelscope
            
            cls._model, cls._tokenizer, _, _ = get_model_modelscope(
                'unaligned_llamba', 
                is_minimal=False
            )
            
            if verbose:
                print("  âœ… Llamba model loaded successfully")
                if torch.cuda.is_available():
                    mem_gb = torch.cuda.memory_allocated() / 1e9
                    print(f"  ğŸ“Š Memory: {mem_gb:.2f}GB allocated")
        
        except Exception as e:
            print(f"  âŒ Failed to load model: {e}")
            raise
    
    @classmethod
    def create_fitness_function(cls, limit: Optional[int] = None, 
                                verbose: bool = False) -> Callable[[List[int]], float]:
        """
        åˆ›å»ºé€‚åº”åº¦å‡½æ•°
        
        Args:
            limit: è¯„ä¼°æ ·æœ¬é™åˆ¶
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        
        Returns:
            fitnesså‡½æ•°: layers -> score
        """
        if cls._model is None:
            raise RuntimeError("Model not initialized. Call initialize() first.")
        
        def fitness_func(layers: List[int]) -> float:
            """è¯„ä¼°å±‚ç»„åˆçš„é€‚åº”åº¦"""
            result = eval_mmlu_with_replacement(
                model=cls._model,
                tokenizer=cls._tokenizer,
                replaced_layers=layers,
                llama_layers_dir=cls._llama_layers_dir,
                limit=limit,
                batch_size=16,
                use_cache=True,
                verbose=verbose
            )
            return result['score'] if result['success'] else 0.0
        
        return fitness_func
    
    @classmethod
    def cleanup(cls):
        """æ¸…ç†èµ„æº"""
        if cls._model is not None:
            del cls._model
            cls._model = None
        if cls._tokenizer is not None:
            del cls._tokenizer
            cls._tokenizer = None
        
        torch.cuda.empty_cache()
        import gc
        gc.collect()


# ==================== ä¾¿æ·å‡½æ•° ====================

def create_mmlu_fitness(llama_layers_dir: str,
                       limit: Optional[int] = None,
                       gpu_id: int = None,
                       verbose: bool = True) -> Callable[[List[int]], float]:
    """
    ä¸€ç«™å¼åˆ›å»ºMMLUé€‚åº”åº¦å‡½æ•°
    
    Args:
        llama_layers_dir: Llamaå±‚ç›®å½•
        limit: è¯„ä¼°æ ·æœ¬é™åˆ¶
        gpu_id: GPU ID
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        fitnesså‡½æ•°
    """
    # åˆå§‹åŒ–ï¼ˆå¦‚æœè¿˜æ²¡åˆå§‹åŒ–ï¼‰
    MMLUFitnessFunction.initialize(
        llama_layers_dir=llama_layers_dir,
        gpu_id=gpu_id,
        verbose=verbose
    )
    
    # åˆ›å»ºfitnesså‡½æ•°
    return MMLUFitnessFunction.create_fitness_function(
        limit=limit,
        verbose=verbose
    )


# ==================== æµ‹è¯•ä»£ç  ====================

def test_simple():
    """ç®€å•æµ‹è¯•"""
    print("\n" + "="*70)
    print("æµ‹è¯•MMLUé€‚åº”åº¦å‡½æ•°")
    print("="*70)
    
    llama_layers_dir = "/home/huzhuangfei/Code/GandA/Gather-and-Aggregate/extracted_llama_layers"
    
    # åˆ›å»ºfitnesså‡½æ•°ï¼ˆlimit=10ï¼Œå¿«é€Ÿæµ‹è¯•ï¼‰
    print("\nåˆ›å»ºfitnesså‡½æ•°...")
    fitness_func = create_mmlu_fitness(
        llama_layers_dir=llama_layers_dir,
        limit=10,
        gpu_id=3,  # ä½¿ç”¨GPU 3
        verbose=True
    )
    
    # æµ‹è¯•1: å•å±‚
    print(f"\n{'='*70}")
    print("æµ‹è¯•1: è¯„ä¼° [17]")
    print("="*70)
    score1 = fitness_func([17])
    print(f"åˆ†æ•°: {score1:.4f}")
    
    # æµ‹è¯•2: 2å±‚
    print(f"\n{'='*70}")
    print("æµ‹è¯•2: è¯„ä¼° [13, 17]")
    print("="*70)
    score2 = fitness_func([13, 17])
    print(f"åˆ†æ•°: {score2:.4f}")
    
    # æ¸…ç†
    print("\næ¸…ç†èµ„æº...")
    MMLUFitnessFunction.cleanup()
    
    print(f"\n{'='*70}")
    print("âœ… æµ‹è¯•å®Œæˆ")
    print("="*70)


if __name__ == "__main__":
    test_simple()

