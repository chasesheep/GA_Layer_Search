#!/usr/bin/env python3
"""
åˆ›å»ºæ›¿æ¢å±‚åçš„å®Œæ•´æ¨¡å‹checkpoint

è¿™ä¸ªè„šæœ¬å°†Llambaæ¨¡å‹å’ŒæŒ‡å®šçš„Llamaå±‚ç»„åˆï¼Œåˆ›å»ºä¸€ä¸ªå¯ä»¥ç›´æ¥åŠ è½½ä½¿ç”¨çš„å®Œæ•´æ¨¡å‹checkpointã€‚

ç”¨æ³•:
    python create_replaced_model_checkpoint.py \
        --layers 11 13 17 21 \
        --output_dir ./checkpoints/llamba_replaced_11_13_17_21 \
        --llama_layers_dir ../extracted_llama_layers

ç”Ÿæˆçš„checkpointå¯ä»¥ç›´æ¥åŠ è½½:
    model = torch.load('checkpoints/llamba_replaced_11_13_17_21/model.pt')
"""

import os
import sys
import torch
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import List

def get_memory_info():
    """è·å–GPUå†…å­˜ä¿¡æ¯"""
    if torch.cuda.is_available():
        return {
            'allocated_gb': torch.cuda.memory_allocated() / 1e9,
            'reserved_gb': torch.cuda.memory_reserved() / 1e9,
        }
    return None

def print_memory_info(stage=""):
    """æ‰“å°å†…å­˜ä¿¡æ¯"""
    mem_info = get_memory_info()
    if mem_info:
        print(f"  ğŸ’¾ GPU Memory {stage}:")
        print(f"       - Allocated: {mem_info['allocated_gb']:.2f}GB")
        print(f"       - Reserved: {mem_info['reserved_gb']:.2f}GB")

def load_llamba_model(device='cuda'):
    """åŠ è½½Llambaæ¨¡å‹ï¼ˆæœªå¯¹é½ç‰ˆæœ¬ï¼‰"""
    print("ğŸ“¥ Loading Llamba model (unaligned)...")
    
    try:
        # æ·»åŠ modelsæ¨¡å—è·¯å¾„ - å°è¯•å¤šç§å¯èƒ½çš„ä½ç½®
        import sys
        from pathlib import Path
        
        # å¯èƒ½çš„Gather-and-Aggregateç›®å½•ä½ç½®
        possible_paths = [
            Path(__file__).parent.parent.parent / 'GandA' / 'Gather-and-Aggregate',  # å¼€å‘ç¯å¢ƒ
            Path(__file__).parent.parent.parent / 'Gather-and-Aggregate',  # å…¶ä»–æƒ…å†µ
            Path(__file__).parent.parent,  # å¦‚æœmodelså°±åœ¨ä¸Šçº§ç›®å½•
            Path.cwd(),  # å½“å‰å·¥ä½œç›®å½•
        ]
        
        gather_dir = None
        for path in possible_paths:
            if (path / 'models').exists():
                gather_dir = path
                print(f"    â„¹ï¸  Found models directory at: {gather_dir}")
                sys.path.insert(0, str(gather_dir))
                break
        
        if gather_dir is None:
            print(f"    âš ï¸  Warning: Could not find models directory")
            print(f"    ğŸ’¡ Tip: Make sure 'models' directory (containing llamba.py) is accessible")
            print(f"    ğŸ’¡ You can:")
            print(f"       1. Run from the original GandA/Gather-and-Aggregate directory")
            print(f"       2. Copy the 'models' directory to this project")
            print(f"       3. Set PYTHONPATH to include the directory containing 'models'")
        
        from modelscope_utils import get_model_modelscope
        model, tokenizer, num_heads, head_dim = get_model_modelscope('unaligned_llamba', is_minimal=False)
        
        print("    âœ… Llamba model loaded successfully")
        print_memory_info("(after loading Llamba)")
        
        return model, tokenizer, num_heads, head_dim
    except Exception as e:
        print(f"    âŒ Failed to load Llamba model: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None, None

def load_extracted_layer(layers_dir, layer_idx, device='cuda'):
    """åŠ è½½é¢„æå–çš„Llamaå±‚"""
    layer_path = Path(layers_dir) / f"layer_{layer_idx:02d}.pt"
    
    if not layer_path.exists():
        raise FileNotFoundError(f"Layer file not found: {layer_path}")
    
    print(f"    ğŸ“‚ Loading Llama layer {layer_idx} from {layer_path}")
    layer = torch.load(layer_path, map_location='cpu')
    layer = layer.to(device)
    
    return layer

def load_extracted_rotary_emb(layers_dir, device='cuda'):
    """åŠ è½½é¢„æå–çš„rotary_emb"""
    rotary_emb_path = Path(layers_dir) / "rotary_emb.pt"
    
    if not rotary_emb_path.exists():
        raise FileNotFoundError(f"Rotary embedding file not found: {rotary_emb_path}")
    
    print(f"    ğŸ“‚ Loading Llama rotary_emb from {rotary_emb_path}")
    rotary_emb = torch.load(rotary_emb_path, map_location='cpu')
    rotary_emb = rotary_emb.to(device)
    
    return rotary_emb

def replace_layers_in_model(model, replaced_layers: List[int], llama_layers_dir, device='cuda'):
    """
    åœ¨æ¨¡å‹ä¸­æ›¿æ¢æŒ‡å®šçš„å±‚
    
    Args:
        model: Llambaæ¨¡å‹
        replaced_layers: è¦æ›¿æ¢çš„å±‚ç´¢å¼•åˆ—è¡¨
        llama_layers_dir: Llamaå±‚æ–‡ä»¶ç›®å½•
        device: è®¾å¤‡
    
    Returns:
        æ›¿æ¢åçš„æ¨¡å‹
    """
    print(f"\nğŸ”„ Replacing {len(replaced_layers)} layers: {replaced_layers}")
    
    # æ›¿æ¢rotary_emb
    print("\n  ğŸ”„ Replacing rotary_emb...")
    llama_rotary_emb = load_extracted_rotary_emb(llama_layers_dir, device=device)
    model.backbone.rotary_emb = llama_rotary_emb
    print("    âœ… rotary_emb replaced")
    
    # æ›¿æ¢æŒ‡å®šçš„å±‚
    print(f"\n  ğŸ”„ Replacing {len(replaced_layers)} transformer layers...")
    for layer_idx in replaced_layers:
        llama_layer = load_extracted_layer(llama_layers_dir, layer_idx, device=device)
        model.backbone.layers[layer_idx] = llama_layer
        print(f"    âœ… Layer {layer_idx} replaced")
        
        # æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()
    
    print("\n  âœ… All layers replaced successfully")
    print_memory_info("(after replacement)")
    
    return model

def save_replaced_model_checkpoint(model, tokenizer, replaced_layers, output_dir, 
                                   num_heads=None, head_dim=None, metadata=None):
    """
    ä¿å­˜æ›¿æ¢åçš„æ¨¡å‹checkpoint
    
    Args:
        model: æ›¿æ¢åçš„æ¨¡å‹
        tokenizer: tokenizer
        replaced_layers: æ›¿æ¢çš„å±‚ç´¢å¼•
        output_dir: è¾“å‡ºç›®å½•
        num_heads: æ³¨æ„åŠ›å¤´æ•°
        head_dim: å¤´ç»´åº¦
        metadata: é¢å¤–çš„å…ƒæ•°æ®
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ’¾ Saving checkpoint to: {output_path}")
    
    # ä¿å­˜state_dictï¼ˆä¸»è¦æ–¹å¼ï¼Œé¿å…pickle lambdaé—®é¢˜ï¼‰
    state_dict_path = output_path / "model_state_dict.pt"
    print(f"  ğŸ“ Saving state_dict to: {state_dict_path}")
    torch.save(model.state_dict(), state_dict_path)
    print(f"    âœ… State dict saved ({state_dict_path.stat().st_size / 1e9:.2f} GB)")
    
    # å°è¯•ä¿å­˜å®Œæ•´æ¨¡å‹ï¼ˆå¯èƒ½å¤±è´¥due to lambdaï¼‰
    model_path = output_path / "model.pt"
    print(f"  ğŸ“ Saving complete model to: {model_path}")
    try:
        torch.save(model, model_path)
        print(f"    âœ… Model saved ({model_path.stat().st_size / 1e9:.2f} GB)")
    except Exception as e:
        print(f"    âš ï¸  Complete model save failed (lambda pickle issue): {e}")
        print(f"    ğŸ’¡ Use model_state_dict.pt instead (it's complete and works)")
    
    # ä¿å­˜tokenizer
    tokenizer_path = output_path / "tokenizer"
    print(f"  ğŸ“ Saving tokenizer to: {tokenizer_path}")
    tokenizer.save_pretrained(tokenizer_path)
    print(f"    âœ… Tokenizer saved")
    
    # ä¿å­˜å…ƒæ•°æ®
    checkpoint_metadata = {
        'replaced_layers': replaced_layers,
        'num_replaced_layers': len(replaced_layers),
        'total_layers': 32,
        'base_model': 'Llamba (unaligned)',
        'replacement_source': 'Llama',
        'num_heads': num_heads,
        'head_dim': head_dim,
        'creation_time': datetime.now().isoformat(),
        'device': str(next(model.parameters()).device),
    }
    
    if metadata:
        checkpoint_metadata.update(metadata)
    
    metadata_path = output_path / "checkpoint_info.json"
    print(f"  ğŸ“ Saving metadata to: {metadata_path}")
    with open(metadata_path, 'w') as f:
        json.dump(checkpoint_metadata, f, indent=2)
    print(f"    âœ… Metadata saved")
    
    # åˆ›å»ºREADME
    readme_path = output_path / "README.txt"
    with open(readme_path, 'w') as f:
        f.write(f"""
Replaced Model Checkpoint
=========================

Base Model: Llamba (unaligned, 8B)
Replacement Source: Llama layers
Replaced Layers: {replaced_layers}
Number of Replaced Layers: {len(replaced_layers)}
Total Layers: 32
Creation Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

Files:
------
- model.pt              : Complete model (can be loaded directly with torch.load)
- model_state_dict.pt   : Model state dict (requires model architecture to load)
- tokenizer/            : Tokenizer files
- checkpoint_info.json  : Detailed metadata
- README.txt            : This file

Usage:
------
# Load complete model
import torch
model = torch.load('model.pt')
model.eval()

# Or load from state dict
from modelscope_utils import get_model_modelscope
model, tokenizer, _, _ = get_model_modelscope('unaligned_llamba')
model.load_state_dict(torch.load('model_state_dict.pt'))

# Load tokenizer
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('./tokenizer')

# Run inference
inputs = tokenizer("Hello world", return_tensors="pt")
outputs = model(**inputs)

For evaluation:
---------------
cd /path/to/GA_Layer_Search/model_preparation
python test_specific_combination.py --checkpoint /path/to/this/checkpoint

""")
    print(f"    âœ… README saved")
    
    print(f"\nâœ… Checkpoint saved successfully!")
    print(f"   Total size: {sum(f.stat().st_size for f in output_path.rglob('*') if f.is_file()) / 1e9:.2f} GB")

def main():
    parser = argparse.ArgumentParser(
        description='Create a checkpoint of Llamba model with replaced Llama layers',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Create checkpoint for layers 11, 13, 17, 21
  python create_replaced_model_checkpoint.py --layers 11 13 17 21 --output_dir ./checkpoints/replaced_11_13_17_21
  
  # Specify custom Llama layers directory
  python create_replaced_model_checkpoint.py --layers 13 17 --llama_layers_dir /path/to/extracted_llama_layers --output_dir ./checkpoints/replaced_13_17
  
  # Use specific GPU
  python create_replaced_model_checkpoint.py --layers 10 14 17 30 --gpu 1 --output_dir ./checkpoints/replaced_10_14_17_30
        """
    )
    
    parser.add_argument('--layers', nargs='+', type=int, required=True,
                       help='Layer indices to replace (e.g., 11 13 17 21)')
    parser.add_argument('--llama_layers_dir', type=str, 
                       default='../extracted_llama_layers',
                       help='Directory containing extracted Llama layers (default: ../extracted_llama_layers)')
    parser.add_argument('--output_dir', type=str, required=True,
                       help='Output directory for checkpoint (e.g., ./checkpoints/replaced_11_13_17_21)')
    parser.add_argument('--gpu', type=int, default=0,
                       help='GPU ID to use (default: 0)')
    parser.add_argument('--description', type=str, default='',
                       help='Additional description for this checkpoint')
    parser.add_argument('--score', type=float, default=None,
                       help='MMLU score of this configuration (if known)')
    
    args = parser.parse_args()
    
    # è®¾ç½®GPU
    os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu)
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    print("="*80)
    print("ğŸš€ Create Replaced Model Checkpoint")
    print("="*80)
    print(f"\nğŸ“‹ Configuration:")
    print(f"   Replaced layers: {args.layers}")
    print(f"   Number of layers: {len(args.layers)}")
    print(f"   Llama layers dir: {args.llama_layers_dir}")
    print(f"   Output directory: {args.output_dir}")
    print(f"   GPU: {args.gpu}")
    print(f"   Device: {device}")
    
    if args.description:
        print(f"   Description: {args.description}")
    if args.score:
        print(f"   MMLU Score: {args.score:.4f}")
    
    # æ£€æŸ¥Llamaå±‚ç›®å½•
    llama_layers_path = Path(args.llama_layers_dir)
    if not llama_layers_path.exists():
        print(f"\nâŒ Error: Llama layers directory not found: {llama_layers_path}")
        print(f"   Please run extract_layers.py first to extract Llama layers.")
        return 1
    
    # æ£€æŸ¥æ‰€æœ‰éœ€è¦çš„å±‚æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    missing_layers = []
    for layer_idx in args.layers:
        layer_path = llama_layers_path / f"layer_{layer_idx:02d}.pt"
        if not layer_path.exists():
            missing_layers.append(layer_idx)
    
    if missing_layers:
        print(f"\nâŒ Error: Missing layer files for layers: {missing_layers}")
        return 1
    
    print(f"\nâœ… All required Llama layer files found")
    
    try:
        # åŠ è½½Llambaæ¨¡å‹
        model, tokenizer, num_heads, head_dim = load_llamba_model(device=device)
        if model is None:
            return 1
        
        # æ›¿æ¢å±‚
        model = replace_layers_in_model(
            model, 
            args.layers, 
            args.llama_layers_dir, 
            device=device
        )
        
        # ä¿å­˜checkpoint
        metadata = {}
        if args.description:
            metadata['description'] = args.description
        if args.score:
            metadata['mmlu_score'] = args.score
        
        save_replaced_model_checkpoint(
            model=model,
            tokenizer=tokenizer,
            replaced_layers=args.layers,
            output_dir=args.output_dir,
            num_heads=num_heads,
            head_dim=head_dim,
            metadata=metadata
        )
        
        print("\n" + "="*80)
        print("âœ… Checkpoint creation completed successfully!")
        print("="*80)
        print(f"\nğŸ“ Checkpoint location: {Path(args.output_dir).absolute()}")
        print(f"\nğŸ¯ Next steps:")
        print(f"   1. Test the checkpoint:")
        print(f"      python test_checkpoint.py --checkpoint {args.output_dir}")
        print(f"   2. Load in your code:")
        print(f"      model = torch.load('{args.output_dir}/model.pt')")
        print()
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())
