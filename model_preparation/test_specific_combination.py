#!/usr/bin/env python3
"""
æµ‹è¯•ç‰¹å®šå±‚ç»„åˆçš„MMLUæ€§èƒ½
ä¸“é—¨ç”¨äºéªŒè¯æŒ‡å®šçš„å±‚æ›¿æ¢ç»„åˆ
"""

import os
import sys
import json
import torch
import time
from datetime import datetime
from pathlib import Path
import warnings
import logging
import gc
import io
from contextlib import redirect_stderr

# è­¦å‘Šå’Œæ—¥å¿—æŠ‘åˆ¶
warnings.filterwarnings("ignore")
logging.getLogger().setLevel(logging.CRITICAL)
logging.getLogger("transformers").setLevel(logging.CRITICAL)
logging.getLogger("datasets").setLevel(logging.CRITICAL)
logging.getLogger("lm_eval").setLevel(logging.CRITICAL)
logging.getLogger("modelscope").setLevel(logging.CRITICAL)
logging.getLogger("huggingface_hub").setLevel(logging.CRITICAL)
logging.getLogger("urllib3").setLevel(logging.CRITICAL)
logging.getLogger("requests").setLevel(logging.CRITICAL)
logging.getLogger("torch").setLevel(logging.CRITICAL)
logging.getLogger("torch.cuda").setLevel(logging.CRITICAL)

warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", message=".*trust_remote_code.*")
warnings.filterwarnings("ignore", message=".*weights_only.*")
warnings.filterwarnings("ignore", message=".*huggingface-hub.*")
warnings.filterwarnings("ignore", message=".*pretrained.*")
warnings.filterwarnings("ignore", message=".*already-initialized.*")
warnings.filterwarnings("ignore", message=".*loading script.*")
warnings.filterwarnings("ignore", message=".*Parquet.*")

os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"
os.environ["HF_HUB_DISABLE_TELEMETRY"] = "1"
os.environ["TRANSFORMERS_VERBOSITY"] = "error"
os.environ["DATASETS_VERBOSITY"] = "error"
os.environ["PYTHONWARNINGS"] = "ignore"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

def get_memory_info():
    """è·å–GPUå†…å­˜ä¿¡æ¯"""
    if torch.cuda.is_available():
        device = torch.cuda.current_device()
        return {
            'allocated_gb': torch.cuda.memory_allocated(device) / 1e9,
            'reserved_gb': torch.cuda.memory_reserved(device) / 1e9,
            'free_gb': (torch.cuda.get_device_properties(device).total_memory - torch.cuda.memory_reserved(device)) / 1e9
        }
    return None

def force_memory_cleanup():
    """å¼ºåˆ¶æ¸…ç†GPUå†…å­˜"""
    torch.cuda.empty_cache()
    gc.collect()
    torch.cuda.synchronize()

def print_memory_status(stage=""):
    """æ‰“å°å†…å­˜çŠ¶æ€"""
    mem_info = get_memory_info()
    if mem_info:
        print(f"  ğŸ“Š Memory {stage}: {mem_info['allocated_gb']:.2f}GB allocated, {mem_info['free_gb']:.2f}GB free")

def load_llamba_model():
    """åŠ è½½Llambaæ¨¡å‹"""
    print("\nğŸ§ª Loading Llamba model...")
    
    try:
        from modelscope_utils import get_model_modelscope
        
        print("  - Loading Llamba-8B-unaligned model...")
        llamba_model, llamba_tokenizer, _, _ = get_model_modelscope('unaligned_llamba', is_minimal=False)
        print("    âœ… Llamba model loaded successfully")
        
        mem_info = get_memory_info()
        if mem_info:
            print(f"    ğŸ“Š Memory after loading: {mem_info['allocated_gb']:.2f}GB allocated")
        
        return llamba_model, llamba_tokenizer
        
    except Exception as e:
        print(f"âŒ Llamba model loading failed: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def load_extracted_layer(layers_dir, layer_idx, device='cuda'):
    """åŠ è½½é¢„æå–çš„å±‚æ–‡ä»¶"""
    layers_path = Path(layers_dir)
    layer_path = layers_path / f"layer_{layer_idx:02d}.pt"
    
    if not layer_path.exists():
        raise FileNotFoundError(f"Layer {layer_idx} not found at {layer_path}")
    
    layer = torch.load(layer_path, map_location=device)
    return layer

def load_extracted_rotary_emb(layers_dir, device='cuda'):
    """åŠ è½½é¢„æå–çš„rotary_emb"""
    layers_path = Path(layers_dir)
    rotary_emb_path = layers_path / "rotary_emb.pt"
    
    if not rotary_emb_path.exists():
        raise FileNotFoundError(f"Rotary embeddings not found at {rotary_emb_path}")
    
    rotary_emb = torch.load(rotary_emb_path, map_location=device)
    return rotary_emb

def eval_mmlu_with_replacement(model, tokenizer, replaced_layers, llama_layers_dir, 
                              limit=None, batch_size=16, use_cache=True):
    """
    ä½¿ç”¨æŒ‡å®šå±‚æ›¿æ¢è¿›è¡ŒMMLUè¯„ä¼°
    """
    limit_str = "full" if limit is None else str(limit)
    print(f"\n  â³ MMLU evaluation with layers {replaced_layers} (limit={limit_str})...")
    
    try:
        from modelscope_utils import run_eval
        
        # ä¿å­˜åŸå§‹çŠ¶æ€
        original_layers = {}
        original_rotary_emb = None
        if hasattr(model.backbone, 'rotary_emb'):
            original_rotary_emb = model.backbone.rotary_emb
        
        # åŠ è½½å¹¶æ›¿æ¢æŒ‡å®šå±‚
        llama_rotary_emb = load_extracted_rotary_emb(llama_layers_dir, device=model.device)
        model.backbone.rotary_emb = llama_rotary_emb
        
        # å­˜å‚¨åŠ è½½çš„Llamaå±‚ï¼Œç”¨äºåç»­æ¸…ç†
        loaded_llama_layers = []
        for layer_idx in replaced_layers:
            original_layers[layer_idx] = model.backbone.layers[layer_idx]
            llama_layer = load_extracted_layer(llama_layers_dir, layer_idx, device=model.device)
            model.backbone.layers[layer_idx] = llama_layer
            loaded_llama_layers.append(llama_layer)
        
        # ç«‹å³æ¸…ç†åŠ è½½çš„Llamaå±‚ï¼Œé¿å…ç´¯ç§¯æ˜¾å­˜å ç”¨
        del llama_rotary_emb
        for llama_layer in loaded_llama_layers:
            del llama_layer
        torch.cuda.empty_cache()
        gc.collect()
        
        # ç›‘æ§å†…å­˜çŠ¶æ€
        print_memory_status("(after layer replacement)")
        
        start_time = time.time()
        
        # è¿è¡Œè¯„ä¼°ï¼Œé‡å®šå‘stderrä»¥æŠ‘åˆ¶warning
        old_stderr = sys.stderr
        sys.stderr = io.StringIO()
        
        try:
            results = run_eval(
                model=model, 
                tokenizer=tokenizer, 
                tasks=["mmlu"],
                limit=limit,
                batch_size=batch_size,
                cache_requests=use_cache
            )
        finally:
            # æ¢å¤åŸå§‹stderr
            sys.stderr = old_stderr
        
        # è¯„ä¼°å®Œæˆåç«‹å³æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()
        gc.collect()
        torch.cuda.synchronize()
        eval_time = time.time() - start_time
        
        # æ¢å¤åŸå§‹çŠ¶æ€
        for layer_idx in replaced_layers:
            model.backbone.layers[layer_idx] = original_layers[layer_idx]
        if original_rotary_emb is not None:
            model.backbone.rotary_emb = original_rotary_emb
        
        if 'results' in results and 'mmlu' in results['results']:
            mmlu_score = results['results']['mmlu']['acc,none']
            print(f"  âœ… MMLU completed in {eval_time:.2f}s: {mmlu_score:.3f}")
            
            return {
                'replaced_layers': replaced_layers,
                'score': mmlu_score,
                'time': eval_time,
                'success': True
            }
        else:
            print(f"  âŒ No MMLU results found")
            return {
                'replaced_layers': replaced_layers,
                'score': 0.0,
                'time': 0.0,
                'success': False,
                'error': "No MMLU results found"
            }
        
    except Exception as e:
        print(f"  âŒ MMLU evaluation failed: {e}")
        import traceback
        traceback.print_exc()
        return {
            'replaced_layers': replaced_layers,
            'score': 0.0,
            'time': 0.0,
            'success': False,
            'error': str(e)
        }

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Test specific layer combination')
    parser.add_argument('--layers', type=int, nargs='+', required=True,
                       help='Layers to replace (e.g., --layers 10 14 17 30)')
    parser.add_argument('--batch_size', type=int, default=16,
                       help='Batch size for evaluation (default: 16)')
    parser.add_argument('--limit', type=int, default=None,
                       help='Limit samples per task (default: None for full MMLU)')
    parser.add_argument('--gpu_id', type=int, default=0,
                       help='GPU ID to use (default: 0)')
    
    args = parser.parse_args()
    
    # è®¾ç½®GPU
    os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu_id)
    
    # æå–å±‚è·¯å¾„
    llama_layers_dir = "/home/huzhuangfei/Code/GandA/Gather-and-Aggregate/extracted_llama_layers"
    
    print(f"\n{'='*80}")
    print(f"CONFIGURATION")
    print(f"{'='*80}")
    print(f"Layers to replace: {args.layers}")
    print(f"Batch size: {args.batch_size}")
    print(f"Limit: {args.limit if args.limit else 'None (full MMLU)'}")
    print(f"GPU ID: {args.gpu_id}")
    print(f"Llama layers path: {llama_layers_dir}")
    print(f"{'='*80}\n")
    
    # åŠ è½½Llambaæ¨¡å‹
    model, tokenizer = load_llamba_model()
    if model is None or tokenizer is None:
        print("\nâŒ Failed to load Llamba model")
        sys.exit(1)
    
    print_memory_status("after model loading")
    
    # è¿è¡Œè¯„ä¼°
    print(f"\n{'='*80}")
    print(f"TESTING COMBINATION: {sorted(args.layers)}")
    print(f"{'='*80}")
    
    result = eval_mmlu_with_replacement(
        model=model,
        tokenizer=tokenizer,
        replaced_layers=sorted(args.layers),
        llama_layers_dir=llama_layers_dir,
        limit=args.limit,
        batch_size=args.batch_size,
        use_cache=True
    )
    
    if result['success']:
        # ä¿å­˜ç»“æœ
        output_file = f"combination_test_{'_'.join(map(str, sorted(args.layers)))}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        result['batch_size'] = args.batch_size
        result['limit'] = args.limit
        result['timestamp'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        with open(output_file, 'w') as f:
            json.dump(result, f, indent=2)
        
        print(f"\n{'='*80}")
        print(f"FINAL RESULT")
        print(f"{'='*80}")
        print(f"Layers: {result['replaced_layers']}")
        print(f"MMLU Accuracy: {result['score']:.4f}")
        print(f"Evaluation Time: {result['time']:.2f}s")
        print(f"Results saved to: {output_file}")
        print(f"{'='*80}\n")
    else:
        print(f"\nâŒ Evaluation failed: {result.get('error', 'Unknown error')}")
        sys.exit(1)
    
    # æ¸…ç†
    del model
    del tokenizer
    force_memory_cleanup()

if __name__ == "__main__":
    main()
