#!/usr/bin/env python3
"""
æå–Llamaæ¨¡å‹çš„æ¯ä¸€å±‚å’Œrotary_embï¼Œä¿å­˜ä¸ºå•ç‹¬æ–‡ä»¶
é¿å…é‡å¤åŠ è½½æ•´ä¸ªæ¨¡å‹ï¼Œæé«˜å®éªŒæ•ˆç‡

ä½¿ç”¨æ–¹æ³•ï¼š
python extract_layers.py --model_name llama --output_dir ./extracted_layers
python extract_layers.py --model_name unaligned_llamba --output_dir ./extracted_llamba_layers
"""

import os
import sys
import torch
import json
import argparse
from datetime import datetime
from pathlib import Path

# Set CUDA device
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

def get_memory_info():
    """è·å–GPUå†…å­˜ä¿¡æ¯"""
    if torch.cuda.is_available():
        return {
            'allocated_gb': torch.cuda.memory_allocated() / 1e9,
            'reserved_gb': torch.cuda.memory_reserved() / 1e9,
            'max_allocated_gb': torch.cuda.max_memory_allocated() / 1e9
        }
    return None

def print_memory_info(stage=""):
    """æ‰“å°å†…å­˜ä¿¡æ¯"""
    mem_info = get_memory_info()
    if mem_info:
        print(f"    ğŸ“Š Memory {stage}:")
        print(f"       - Allocated: {mem_info['allocated_gb']:.2f}GB")
        print(f"       - Reserved: {mem_info['reserved_gb']:.2f}GB")

def extract_model_layers(model_name, output_dir):
    """
    æå–æ¨¡å‹çš„æ‰€æœ‰å±‚å’Œrotary_emb
    """
    print(f"ğŸš€ Extracting layers from {model_name} model")
    print("=" * 60)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ“ Output directory: {output_path.absolute()}")
    
    try:
        from modelscope_utils import get_model_modelscope
        
        print(f"  - Loading {model_name} model...")
        model, tokenizer, num_heads, head_dim = get_model_modelscope(model_name, is_minimal=False)
        print("    âœ… Model loaded successfully")
        print_memory_info("(after model loading)")
        
        # è·å–æ¨¡å‹ä¿¡æ¯
        total_layers = len(model.backbone.layers)
        print(f"  - Total layers: {total_layers}")
        print(f"  - Number of heads: {num_heads}")
        print(f"  - Head dimension: {head_dim}")
        
        # æå–rotary_emb
        print(f"\nğŸ”„ Extracting rotary_emb...")
        rotary_emb = model.backbone.rotary_emb.cpu()
        rotary_emb_path = output_path / "rotary_emb.pt"
        torch.save(rotary_emb, rotary_emb_path)
        print(f"    âœ… Rotary embeddings saved to: {rotary_emb_path}")
        
        # æå–æ¯ä¸€å±‚
        print(f"\nğŸ”„ Extracting {total_layers} layers...")
        layer_info = {
            'model_name': model_name,
            'total_layers': total_layers,
            'num_heads': num_heads,
            'head_dim': head_dim,
            'extraction_time': datetime.now().isoformat(),
            'layers': []
        }
        
        for layer_idx in range(total_layers):
            print(f"  - Extracting layer {layer_idx}...")
            
            # æå–å±‚å¹¶ç§»åŠ¨åˆ°CPU
            layer = model.backbone.layers[layer_idx].cpu()
            
            # ä¿å­˜å±‚
            layer_path = output_path / f"layer_{layer_idx:02d}.pt"
            torch.save(layer, layer_path)
            
            # è®°å½•å±‚ä¿¡æ¯
            layer_info['layers'].append({
                'layer_idx': layer_idx,
                'file_path': str(layer_path),
                'layer_type': str(type(layer)),
                'parameters': sum(p.numel() for p in layer.parameters()),
                'state_dict_keys': list(layer.state_dict().keys())
            })
            
            print(f"    âœ… Layer {layer_idx} saved to: {layer_path}")
            
            # æ¸…ç†GPUç¼“å­˜
            torch.cuda.empty_cache()
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata_path = output_path / "metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(layer_info, f, indent=2, ensure_ascii=False)
        print(f"    âœ… Metadata saved to: {metadata_path}")
        
        # åˆ é™¤æ¨¡å‹ä»¥é‡Šæ”¾å†…å­˜
        del model
        torch.cuda.empty_cache()
        print("    âœ… Model deleted, memory freed")
        print_memory_info("(after model deletion)")
        
        return True, layer_info
        
    except Exception as e:
        print(f"âŒ Layer extraction failed: {e}")
        import traceback
        traceback.print_exc()
        return False, None

def verify_extracted_layers(output_dir):
    """
    éªŒè¯æå–çš„å±‚æ–‡ä»¶
    """
    print(f"\nğŸ” Verifying extracted layers in {output_dir}")
    print("=" * 60)
    
    output_path = Path(output_dir)
    metadata_path = output_path / "metadata.json"
    
    if not metadata_path.exists():
        print("âŒ Metadata file not found")
        return False
    
    # åŠ è½½å…ƒæ•°æ®
    with open(metadata_path, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    
    print(f"  - Model: {metadata['model_name']}")
    print(f"  - Total layers: {metadata['total_layers']}")
    print(f"  - Extraction time: {metadata['extraction_time']}")
    
    # éªŒè¯rotary_emb
    rotary_emb_path = output_path / "rotary_emb.pt"
    if rotary_emb_path.exists():
        rotary_emb = torch.load(rotary_emb_path, map_location='cpu')
        print(f"    âœ… Rotary embeddings: {type(rotary_emb)}")
    else:
        print("    âŒ Rotary embeddings not found")
        return False
    
    # éªŒè¯æ¯ä¸€å±‚
    missing_layers = []
    for layer_info in metadata['layers']:
        layer_path = Path(layer_info['file_path'])
        if layer_path.exists():
            try:
                layer = torch.load(layer_path, map_location='cpu')
                print(f"    âœ… Layer {layer_info['layer_idx']:2d}: {type(layer)}")
            except Exception as e:
                print(f"    âŒ Layer {layer_info['layer_idx']:2d}: Failed to load - {e}")
                missing_layers.append(layer_info['layer_idx'])
        else:
            print(f"    âŒ Layer {layer_info['layer_idx']:2d}: File not found")
            missing_layers.append(layer_info['layer_idx'])
    
    if missing_layers:
        print(f"âŒ Missing layers: {missing_layers}")
        return False
    else:
        print("âœ… All layers verified successfully")
        return True

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Extract model layers and rotary embeddings')
    parser.add_argument('--model_name', type=str, default='llama', 
                       choices=['llama', 'unaligned_llamba'],
                       help='Model name to extract layers from')
    parser.add_argument('--output_dir', type=str, default='./extracted_layers',
                       help='Output directory for extracted layers')
    parser.add_argument('--verify', action='store_true',
                       help='Verify extracted layers')
    
    args = parser.parse_args()
    
    print("ğŸš€ Model Layer Extraction Tool")
    print("=" * 70)
    print(f"Model: {args.model_name}")
    print(f"Output: {args.output_dir}")
    print("=" * 70)
    
    # æ£€æŸ¥GPU
    if not torch.cuda.is_available():
        print("âŒ CUDA not available. Please ensure GPU is accessible.")
        return 1
    
    print(f"âœ… Using GPU: {torch.cuda.get_device_name(0)}")
    print(f"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    # æ¸…ç†GPUç¼“å­˜
    torch.cuda.empty_cache()
    
    if args.verify:
        # éªŒè¯æ¨¡å¼
        success = verify_extracted_layers(args.output_dir)
        return 0 if success else 1
    else:
        # æå–æ¨¡å¼
        success, metadata = extract_model_layers(args.model_name, args.output_dir)
        
        if not success:
            print("âŒ Layer extraction failed.")
            return 1
        
        print("\n" + "=" * 70)
        print("ğŸ‰ LAYER EXTRACTION COMPLETED")
        print("=" * 70)
        
        print(f"ğŸ“Š Summary:")
        print(f"  - Model: {metadata['model_name']}")
        print(f"  - Total layers: {metadata['total_layers']}")
        print(f"  - Output directory: {args.output_dir}")
        
        print(f"\nğŸ’¡ Next steps:")
        print("1. âœ… All layers extracted successfully")
        print("2. ğŸ“ Layers saved as individual .pt files")
        print("3. ğŸ” Use verify mode to check extraction: --verify")
        print("4. ğŸš€ Use extracted layers in replacement experiments")
        
        return 0

if __name__ == "__main__":
    sys.exit(main())
