#!/usr/bin/env python3
"""
åŠ è½½checkpointçš„è¾…åŠ©å‡½æ•°

ç®€åŒ–checkpointåŠ è½½è¿‡ç¨‹
"""

import torch
from pathlib import Path
from transformers import AutoTokenizer


def load_checkpoint(checkpoint_dir, device='cuda'):
    """
    åŠ è½½æ›¿æ¢å±‚åçš„æ¨¡å‹checkpoint
    
    Args:
        checkpoint_dir: checkpointç›®å½•è·¯å¾„
        device: è®¾å¤‡ï¼ˆ'cuda'æˆ–'cpu'ï¼‰
    
    Returns:
        model, tokenizer
    
    Example:
        model, tokenizer = load_checkpoint('model_checkpoints/best_4layer')
        inputs = tokenizer("Hello", return_tensors="pt").to('cuda')
        outputs = model.generate(**inputs, max_new_tokens=50)
    """
    checkpoint_path = Path(checkpoint_dir)
    
    print(f"ğŸ“‚ Loading checkpoint from: {checkpoint_path}")
    
    # å¯¼å…¥å¿…è¦çš„æ¨¡å—
    import sys
    from pathlib import Path
    
    # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°path
    project_root = Path(__file__).parent.parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    from model_preparation.modelscope_utils import get_model_modelscope
    
    # 1. åŠ è½½æ¨¡å‹æ¶æ„
    print("  ğŸ“¥ Loading model architecture...")
    model, _, _, _ = get_model_modelscope('unaligned_llamba', is_minimal=False)
    print("     âœ… Model architecture loaded")
    
    # 2. åŠ è½½æƒé‡
    state_dict_path = checkpoint_path / "model_state_dict.pt"
    if not state_dict_path.exists():
        raise FileNotFoundError(f"State dict not found: {state_dict_path}")
    
    print(f"  ğŸ“¥ Loading weights from: {state_dict_path.name}")
    state_dict = torch.load(state_dict_path, map_location='cpu')
    model.load_state_dict(state_dict)
    print("     âœ… Weights loaded")
    
    # 3. ç§»åˆ°è®¾å¤‡
    model = model.to(device)
    model.eval()
    print(f"     âœ… Model ready on {device}")
    
    # 4. åŠ è½½tokenizer
    tokenizer_path = checkpoint_path / "tokenizer"
    if not tokenizer_path.exists():
        raise FileNotFoundError(f"Tokenizer not found: {tokenizer_path}")
    
    print(f"  ğŸ“¥ Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
    print("     âœ… Tokenizer loaded")
    
    print(f"\nâœ… Checkpoint loaded successfully!")
    
    return model, tokenizer


def quick_test(checkpoint_dir, test_prompt="Hello, how are you?", max_new_tokens=50):
    """
    å¿«é€Ÿæµ‹è¯•checkpoint
    
    Args:
        checkpoint_dir: checkpointç›®å½•
        test_prompt: æµ‹è¯•prompt
        max_new_tokens: ç”Ÿæˆtokenæ•°
    """
    print("="*70)
    print("å¿«é€Ÿæµ‹è¯•Checkpoint")
    print("="*70)
    
    # åŠ è½½
    model, tokenizer = load_checkpoint(checkpoint_dir)
    
    # æ¨ç†
    print(f"\nğŸ“ Test prompt: {test_prompt}")
    inputs = tokenizer(test_prompt, return_tensors="pt").to('cuda')
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
    
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"ğŸ“¤ Generated: {generated_text}")
    print(f"\nâœ… Test passed!")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Load and test checkpoint')
    parser.add_argument('checkpoint_dir', type=str, help='Checkpoint directory')
    parser.add_argument('--prompt', type=str, default="Hello, how are you?", 
                       help='Test prompt')
    parser.add_argument('--max_tokens', type=int, default=50,
                       help='Max new tokens to generate')
    
    args = parser.parse_args()
    
    quick_test(args.checkpoint_dir, args.prompt, args.max_tokens)

