# æ¨¡å‹Checkpointç”Ÿæˆå’Œä½¿ç”¨æŒ‡å—

æœ¬æ–‡æ¡£è¯´æ˜å¦‚ä½•ç”Ÿæˆã€ä¿å­˜å’Œä½¿ç”¨æ›¿æ¢å±‚åçš„å®Œæ•´æ¨¡å‹checkpointã€‚

## ğŸ“‹ æ¦‚è¿°

GAæœç´¢æ‰¾åˆ°çš„æ˜¯"æœ€ä¼˜å±‚ç»„åˆ"ï¼ˆå¦‚ `[11, 13, 17, 21]`ï¼‰ï¼Œä½†è¿™åªæ˜¯é…ç½®ä¿¡æ¯ã€‚è¦å®é™…ä½¿ç”¨è¿™ä¸ªæ¨¡å‹ï¼Œéœ€è¦ï¼š

1. åŠ è½½LlambaåŸºç¡€æ¨¡å‹
2. ç”¨æŒ‡å®šçš„Llamaå±‚æ›¿æ¢ç›¸åº”ä½ç½®
3. ä¿å­˜ä¸ºå®Œæ•´çš„æ¨¡å‹checkpoint

è¿™æ ·ç”Ÿæˆçš„checkpointå¯ä»¥ç›´æ¥åŠ è½½ä½¿ç”¨ï¼Œæ— éœ€æ¯æ¬¡é‡æ–°æ›¿æ¢ã€‚

## ğŸ¯ ä¸ºä»€ä¹ˆéœ€è¦Checkpointï¼Ÿ

**é—®é¢˜**ï¼šGAæœç´¢ç»“æœåªæ˜¯å±‚ç´¢å¼•åˆ—è¡¨ï¼Œå¦‚ `[11, 13, 17, 21]`

**è§£å†³æ–¹æ¡ˆ**ï¼šç”ŸæˆåŒ…å«å®Œæ•´æ¨¡å‹æƒé‡çš„checkpoint

**ä¼˜åŠ¿**ï¼š
- âœ… å³å¼€å³ç”¨ï¼Œæ— éœ€æ¯æ¬¡æ›¿æ¢å±‚
- âœ… å¯ç›´æ¥éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
- âœ… æ–¹ä¾¿åˆ†äº«å’Œå¤ç°ç»“æœ
- âœ… æ”¯æŒæ ‡å‡†çš„æ¨¡å‹åŠ è½½æ¥å£

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼1: åˆ›å»ºå•ä¸ªCheckpoint

```bash
cd model_preparation/

# åˆ›å»º11, 13, 17, 21å±‚æ›¿æ¢çš„checkpoint
python create_replaced_model_checkpoint.py \
    --layers 11 13 17 21 \
    --output_dir ../model_checkpoints/llamba_replaced_11_13_17_21 \
    --description "GAæœç´¢å‘ç°çš„4å±‚æœ€ä¼˜ç»„åˆ" \
    --score 0.5700
```

**å‚æ•°è¯´æ˜**ï¼š
- `--layers`: è¦æ›¿æ¢çš„å±‚ç´¢å¼•
- `--output_dir`: checkpointè¾“å‡ºç›®å½•
- `--description`: æè¿°ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
- `--score`: MMLUåˆ†æ•°ï¼ˆå¯é€‰ï¼‰
- `--llama_layers_dir`: Llamaå±‚æ–‡ä»¶ç›®å½•ï¼ˆé»˜è®¤`../extracted_llama_layers`ï¼‰
- `--gpu`: ä½¿ç”¨çš„GPU IDï¼ˆé»˜è®¤0ï¼‰

### æ–¹å¼2: æ‰¹é‡åˆ›å»ºæ‰€æœ‰æœ€ä¼˜Checkpoint

```bash
cd model_preparation/

# æ‰¹é‡åˆ›å»ºæ‰€æœ‰GAå‘ç°çš„æœ€ä¼˜ç»„åˆ
./create_best_checkpoints.sh
```

è¿™å°†è‡ªåŠ¨åˆ›å»ºä»¥ä¸‹checkpointï¼š
- `llamba_replaced_11_13_17_21` - 4å±‚æœ€ä¼˜ï¼ˆ0.5700ï¼‰
- `llamba_replaced_13_16_17` - 3å±‚æœ€ä¼˜ï¼ˆ0.6542ï¼‰
- `llamba_replaced_13_17` - 2å±‚æœ€ä¼˜ï¼ˆ0.5544ï¼‰
- `llamba_replaced_17` - 1å±‚æœ€ä¼˜ï¼ˆ0.5144ï¼‰
- å…¶ä»–å¤‡é€‰ç»„åˆ...

## ğŸ“‚ Checkpointç»“æ„

ç”Ÿæˆçš„æ¯ä¸ªcheckpointåŒ…å«ï¼š

```
llamba_replaced_11_13_17_21/
â”œâ”€â”€ model.pt                # å®Œæ•´æ¨¡å‹ï¼ˆ~16GBï¼‰â­
â”œâ”€â”€ model_state_dict.pt     # æ¨¡å‹state dictï¼ˆ~16GBï¼‰
â”œâ”€â”€ tokenizer/              # Tokenizeræ–‡ä»¶
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”œâ”€â”€ vocab.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ checkpoint_info.json    # å…ƒæ•°æ®ä¿¡æ¯
â””â”€â”€ README.txt              # ä½¿ç”¨è¯´æ˜
```

### æ–‡ä»¶è¯´æ˜

| æ–‡ä»¶ | å¤§å° | ç”¨é€” |
|------|------|------|
| `model.pt` | ~16GB | å®Œæ•´æ¨¡å‹ï¼Œå¯ç›´æ¥åŠ è½½ â­ |
| `model_state_dict.pt` | ~16GB | State dictï¼Œéœ€è¦æ¨¡å‹æ¶æ„ |
| `tokenizer/` | ~2MB | Tokenizeræ–‡ä»¶ |
| `checkpoint_info.json` | ~1KB | å…ƒæ•°æ®ï¼ˆå±‚é…ç½®ã€åˆ†æ•°ç­‰ï¼‰ |
| `README.txt` | ~1KB | ä½¿ç”¨è¯´æ˜ |

**æ¨èä½¿ç”¨ `model.pt`**ï¼šæœ€ç®€å•ï¼Œç›´æ¥ `torch.load()` å³å¯ã€‚

## ğŸ’» ä½¿ç”¨Checkpoint

### æ–¹æ³•1: ç›´æ¥åŠ è½½å®Œæ•´æ¨¡å‹ï¼ˆæ¨èï¼‰

```python
import torch

# åŠ è½½æ¨¡å‹
model = torch.load('model_checkpoints/llamba_replaced_11_13_17_21/model.pt')
model.eval()
model = model.cuda()  # ç§»åˆ°GPU

# åŠ è½½tokenizer
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('model_checkpoints/llamba_replaced_11_13_17_21/tokenizer')

# æ¨ç†
inputs = tokenizer("Hello world", return_tensors="pt").to('cuda')
with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=50)
    
text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(text)
```

### æ–¹æ³•2: ä»State DictåŠ è½½

```python
import torch
from modelscope_utils import get_model_modelscope

# åŠ è½½æ¨¡å‹æ¶æ„
model, tokenizer, _, _ = get_model_modelscope('unaligned_llamba')

# åŠ è½½æƒé‡
state_dict = torch.load('model_checkpoints/llamba_replaced_11_13_17_21/model_state_dict.pt')
model.load_state_dict(state_dict)

model.eval()
model = model.cuda()
```

### æ–¹æ³•3: æŸ¥çœ‹Checkpointä¿¡æ¯

```python
import json

# è¯»å–å…ƒæ•°æ®
with open('model_checkpoints/llamba_replaced_11_13_17_21/checkpoint_info.json') as f:
    info = json.load(f)

print(f"Replaced layers: {info['replaced_layers']}")
print(f"MMLU score: {info.get('mmlu_score', 'N/A')}")
print(f"Creation time: {info['creation_time']}")
```

## ğŸ§ª æµ‹è¯•Checkpoint

### å¿«é€Ÿæµ‹è¯•ï¼ˆæ¨ç†ï¼‰

```bash
cd model_preparation/

# æµ‹è¯•åŸºæœ¬æ¨ç†åŠŸèƒ½
python test_checkpoint.py --checkpoint ../model_checkpoints/llamba_replaced_11_13_17_21
```

### å®Œæ•´MMLUè¯„ä¼°

```bash
# å¿«é€Ÿè¯„ä¼°ï¼ˆlimit=100ï¼‰
python test_checkpoint.py \
    --checkpoint ../model_checkpoints/llamba_replaced_11_13_17_21 \
    --full_eval \
    --limit 100

# å®Œæ•´MMLUè¯„ä¼°ï¼ˆæ— limitï¼‰
python test_checkpoint.py \
    --checkpoint ../model_checkpoints/llamba_replaced_11_13_17_21 \
    --full_eval
```

## ğŸ“Š ç¤ºä¾‹ï¼šåŸºäºGAæœç´¢ç»“æœåˆ›å»ºCheckpoint

å‡è®¾GAæœç´¢å‘ç°æœ€ä¼˜ç»„åˆæ˜¯ `[11, 13, 17, 21]`ï¼ŒMMLUåˆ†æ•° `0.5700`ï¼š

```bash
cd model_preparation/

# 1. åˆ›å»ºcheckpoint
python create_replaced_model_checkpoint.py \
    --layers 11 13 17 21 \
    --output_dir ../model_checkpoints/best_4layer \
    --description "GAæœç´¢å‘ç°çš„4å±‚æœ€ä¼˜ç»„åˆ" \
    --score 0.5700 \
    --gpu 0

# 2. æµ‹è¯•checkpoint
python test_checkpoint.py \
    --checkpoint ../model_checkpoints/best_4layer \
    --full_eval \
    --limit 50

# 3. ä½¿ç”¨checkpoint
python -c "
import torch
model = torch.load('../model_checkpoints/best_4layer/model.pt')
print('âœ… Model loaded successfully!')
print(f'Device: {next(model.parameters()).device}')
print(f'Total parameters: {sum(p.numel() for p in model.parameters())/1e9:.2f}B')
"
```

## ğŸ’¾ å­˜å‚¨å’Œç®¡ç†

### ç£ç›˜ç©ºé—´éœ€æ±‚

- å•ä¸ªcheckpoint: ~16GB
- 5ä¸ªæœ€ä¼˜ç»„åˆ: ~80GB
- å»ºè®®é¢„ç•™: 100GB+

### ç›®å½•ç»„ç»‡

æ¨èçš„ç›®å½•ç»“æ„ï¼š

```
GA_Layer_Search/
â”œâ”€â”€ extracted_llama_layers/      # Llamaå±‚æ–‡ä»¶ï¼ˆ~40GBï¼‰
â”œâ”€â”€ model_checkpoints/           # ç”Ÿæˆçš„checkpoint
â”‚   â”œâ”€â”€ llamba_replaced_11_13_17_21/  # ~16GB
â”‚   â”œâ”€â”€ llamba_replaced_13_16_17/     # ~16GB
â”‚   â”œâ”€â”€ llamba_replaced_13_17/        # ~16GB
â”‚   â””â”€â”€ ...
â””â”€â”€ model_preparation/           # å·¥å…·è„šæœ¬
    â”œâ”€â”€ create_replaced_model_checkpoint.py
    â”œâ”€â”€ create_best_checkpoints.sh
    â””â”€â”€ test_checkpoint.py
```

### Gitç®¡ç†

**é‡è¦**ï¼šCheckpointæ–‡ä»¶å¾ˆå¤§ï¼ˆ~16GBï¼‰ï¼Œä¸åº”æäº¤åˆ°Gitã€‚

`.gitignore` å·²é…ç½®å¿½ç•¥ï¼š
```gitignore
model_checkpoints/
*.pt
*.pth
```

### åˆ†äº«Checkpoint

å¦‚æœéœ€è¦åˆ†äº«checkpointï¼š

**é€‰é¡¹1: äº‘å­˜å‚¨**
```bash
# å‹ç¼©checkpoint
tar -czf llamba_replaced_11_13_17_21.tar.gz model_checkpoints/llamba_replaced_11_13_17_21/

# ä¸Šä¼ åˆ°äº‘å­˜å‚¨ï¼ˆGoogle Drive, ç™¾åº¦äº‘ç­‰ï¼‰
# æ¥æ”¶æ–¹ä¸‹è½½åè§£å‹å³å¯ä½¿ç”¨
```

**é€‰é¡¹2: æä¾›ç”Ÿæˆå‘½ä»¤**
```bash
# åœ¨READMEä¸­æä¾›å‘½ä»¤ï¼Œè®©ç”¨æˆ·è‡ªå·±ç”Ÿæˆ
python create_replaced_model_checkpoint.py --layers 11 13 17 21 --output_dir ./checkpoints/best
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰å±‚ç»„åˆ

```bash
# åˆ›å»ºè‡ªå·±çš„å±‚ç»„åˆ
python create_replaced_model_checkpoint.py \
    --layers 5 10 15 20 25 30 \
    --output_dir ../model_checkpoints/custom_6layer \
    --description "è‡ªå®šä¹‰6å±‚å‡åŒ€åˆ†å¸ƒ" \
    --gpu 1
```

### æ‰¹é‡åˆ›å»ºç‰¹å®šé…ç½®

```bash
# åˆ›å»º2å±‚ã€3å±‚ã€4å±‚çš„æ‰€æœ‰topç»„åˆ
COMBINATIONS=(
    "13 17"
    "14 17"
    "13 16 17"
    "11 13 17 21"
    "10 14 17 30"
)

for layers in "${COMBINATIONS[@]}"; do
    layers_name=$(echo $layers | tr ' ' '_')
    python create_replaced_model_checkpoint.py \
        --layers $layers \
        --output_dir ../model_checkpoints/combo_${layers_name} \
        --gpu 0
done
```

### åœ¨å…¶ä»–é¡¹ç›®ä¸­ä½¿ç”¨

```python
# åœ¨ä½ çš„é¡¹ç›®ä¸­åŠ è½½checkpoint
import torch
import sys
sys.path.append('/path/to/GA_Layer_Search/model_preparation')

# åŠ è½½æ¨¡å‹
MODEL_PATH = '/path/to/GA_Layer_Search/model_checkpoints/llamba_replaced_11_13_17_21/model.pt'
model = torch.load(MODEL_PATH)
model.eval()

# æ­£å¸¸ä½¿ç”¨
# ...
```

## ğŸ“ æœ€ä½³å®è·µ

### 1. å‘½åè§„èŒƒ

ä½¿ç”¨æ¸…æ™°çš„å‘½åï¼š
- `llamba_replaced_<layers>` - åŸºæœ¬æ ¼å¼
- `llamba_replaced_11_13_17_21` - 4å±‚ç»„åˆ
- `best_4layer` - ç®€çŸ­åˆ«å

### 2. å…ƒæ•°æ®è®°å½•

å§‹ç»ˆè®°å½•ï¼š
- æ›¿æ¢çš„å±‚ç´¢å¼•
- MMLUåˆ†æ•°
- åˆ›å»ºæ—¶é—´
- æè¿°ä¿¡æ¯

### 3. ç‰ˆæœ¬æ§åˆ¶

ä¸ºä¸åŒç‰ˆæœ¬çš„checkpointæ·»åŠ æ ‡è¯†ï¼š
- `llamba_replaced_11_13_17_21_v1`
- `llamba_replaced_11_13_17_21_v2_finetuned`

### 4. æµ‹è¯•éªŒè¯

åˆ›å»ºåç«‹å³æµ‹è¯•ï¼š
```bash
python test_checkpoint.py --checkpoint <path> --full_eval --limit 50
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **GPUå†…å­˜**: åˆ›å»ºcheckpointéœ€è¦~20GB GPUæ˜¾å­˜
2. **ç£ç›˜ç©ºé—´**: æ¯ä¸ªcheckpointçº¦16GB
3. **æ—¶é—´æˆæœ¬**: åˆ›å»ºå•ä¸ªcheckpointçº¦5-10åˆ†é’Ÿ
4. **ä¾èµ–è¦æ±‚**: éœ€è¦å·²æå–çš„Llamaå±‚æ–‡ä»¶
5. **ä¸€è‡´æ€§**: ç¡®ä¿ä½¿ç”¨ç›¸åŒç‰ˆæœ¬çš„æ¨¡å‹å’Œå±‚æ–‡ä»¶

## ğŸ†˜ æ•…éšœæ’é™¤

### é—®é¢˜1: æ‰¾ä¸åˆ°å±‚æ–‡ä»¶

```
FileNotFoundError: Layer file not found: .../layer_11.pt
```

**è§£å†³**ï¼š
```bash
cd model_preparation/
python extract_layers.py --model_name llama --output_dir ../extracted_llama_layers
```

### é—®é¢˜2: GPUå†…å­˜ä¸è¶³

```
CUDA out of memory
```

**è§£å†³**ï¼š
- ä½¿ç”¨æ˜¾å­˜æ›´å¤§çš„GPU
- å…³é—­å…¶ä»–å ç”¨GPUçš„è¿›ç¨‹
- ä½¿ç”¨ `--gpu` å‚æ•°æŒ‡å®šç©ºé—²GPU

### é—®é¢˜3: åŠ è½½checkpointå¤±è´¥

```
RuntimeError: Error loading model.pt
```

**è§£å†³**ï¼š
- æ£€æŸ¥PyTorchç‰ˆæœ¬å…¼å®¹æ€§
- å°è¯•åŠ è½½ `model_state_dict.pt` è€Œé `model.pt`
- ç¡®è®¤checkpointå®Œæ•´æ€§ï¼ˆæ–‡ä»¶å¤§å°æ­£å¸¸ï¼‰

## ğŸ“š å‚è€ƒèµ„æ–™

- **åˆ›å»ºå·¥å…·**: `create_replaced_model_checkpoint.py`
- **æµ‹è¯•å·¥å…·**: `test_checkpoint.py`
- **æ‰¹é‡è„šæœ¬**: `create_best_checkpoints.sh`
- **æ¨¡å‹å‡†å¤‡**: `SETUP.md`
- **é¡¹ç›®ä¸»æ–‡æ¡£**: `README.md`

---

**æ›´æ–°æ—¶é—´**: 2025-10-15  
**ç‰ˆæœ¬**: v1.0

