================================================================================
æ¨¡å‹Checkpointç”Ÿæˆ - å¿«é€Ÿå¼€å§‹
================================================================================

ğŸ“‹ ç›®çš„: å°†GAæœç´¢å‘ç°çš„å±‚ç»„åˆè½¬æ¢ä¸ºå¯ç›´æ¥ä½¿ç”¨çš„å®Œæ•´æ¨¡å‹checkpoint

ğŸ¯ åŸºäºæœç´¢ç»“æœ [11, 13, 17, 21]ï¼ŒMMLUåˆ†æ•° 0.5700

================================================================================
å‰ç½®è¦æ±‚
================================================================================

1. âœ… å·²å®Œæˆæ¨¡å‹å‡†å¤‡ï¼ˆæå–Llamaå±‚æ–‡ä»¶ï¼‰
   cd model_preparation/
   python extract_layers.py --model_name llama --output_dir ../extracted_llama_layers

2. âœ… ç£ç›˜ç©ºé—´å……è¶³ï¼ˆæ¯ä¸ªcheckpointçº¦16GBï¼‰

3. âœ… GPUå¯ç”¨ï¼ˆéœ€è¦18GB+æ˜¾å­˜ï¼‰

================================================================================
æ–¹å¼1: åˆ›å»ºå•ä¸ªCheckpointï¼ˆæ¨èç”¨äºæµ‹è¯•ï¼‰
================================================================================

cd model_preparation/

# åˆ›å»º11, 13, 17, 21å±‚æ›¿æ¢çš„checkpoint
python create_replaced_model_checkpoint.py \
    --layers 11 13 17 21 \
    --output_dir ../model_checkpoints/llamba_replaced_11_13_17_21 \
    --description "GAæœç´¢å‘ç°çš„4å±‚æœ€ä¼˜ç»„åˆ" \
    --score 0.5700 \
    --gpu 0

é¢„è®¡æ—¶é—´: 5-10åˆ†é’Ÿ
è¾“å‡ºå¤§å°: ~16GB
è¾“å‡ºä½ç½®: model_checkpoints/llamba_replaced_11_13_17_21/

================================================================================
æ–¹å¼2: æ‰¹é‡åˆ›å»ºæ‰€æœ‰æœ€ä¼˜Checkpoint
================================================================================

cd model_preparation/

# æ‰¹é‡åˆ›å»ºæ‰€æœ‰GAå‘ç°çš„æœ€ä¼˜ç»„åˆ
./create_best_checkpoints.sh

å°†åˆ›å»º:
- llamba_replaced_11_13_17_21  (4å±‚, 0.5700)
- llamba_replaced_13_16_17     (3å±‚, 0.6542)
- llamba_replaced_13_17        (2å±‚, 0.5544)
- llamba_replaced_17           (1å±‚, 0.5144)
- llamba_replaced_10_14_17_30  (4å±‚å¤‡é€‰)

é¢„è®¡æ—¶é—´: 30-50åˆ†é’Ÿ
æ€»å¤§å°: ~80GB

================================================================================
æµ‹è¯•Checkpoint
================================================================================

cd model_preparation/

# å¿«é€Ÿæ¨ç†æµ‹è¯•
python test_checkpoint.py \
    --checkpoint ../model_checkpoints/llamba_replaced_11_13_17_21

# å®Œæ•´MMLUè¯„ä¼°ï¼ˆå¿«é€Ÿç‰ˆï¼Œlimit=100ï¼‰
python test_checkpoint.py \
    --checkpoint ../model_checkpoints/llamba_replaced_11_13_17_21 \
    --full_eval \
    --limit 100

# å®Œæ•´MMLUè¯„ä¼°ï¼ˆå®Œæ•´ç‰ˆï¼‰
python test_checkpoint.py \
    --checkpoint ../model_checkpoints/llamba_replaced_11_13_17_21 \
    --full_eval

================================================================================
ä½¿ç”¨Checkpoint
================================================================================

æ–¹æ³•1: ç›´æ¥åŠ è½½ï¼ˆPythonï¼‰
---------------------------
import torch
from transformers import AutoTokenizer

# åŠ è½½æ¨¡å‹
model = torch.load('model_checkpoints/llamba_replaced_11_13_17_21/model.pt')
model.eval()
model = model.cuda()

# åŠ è½½tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    'model_checkpoints/llamba_replaced_11_13_17_21/tokenizer'
)

# æ¨ç†
inputs = tokenizer("Hello", return_tensors="pt").to('cuda')
outputs = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(outputs[0]))

æ–¹æ³•2: æŸ¥çœ‹Checkpointä¿¡æ¯
---------------------------
import json

with open('model_checkpoints/llamba_replaced_11_13_17_21/checkpoint_info.json') as f:
    info = json.load(f)
    print(f"Replaced layers: {info['replaced_layers']}")
    print(f"MMLU score: {info['mmlu_score']}")

================================================================================
Checkpointç›®å½•ç»“æ„
================================================================================

model_checkpoints/llamba_replaced_11_13_17_21/
â”œâ”€â”€ model.pt                # å®Œæ•´æ¨¡å‹ (~16GB) â­ ä¸»è¦ä½¿ç”¨è¿™ä¸ª
â”œâ”€â”€ model_state_dict.pt     # State dict (~16GB)
â”œâ”€â”€ tokenizer/              # Tokenizeræ–‡ä»¶
â”œâ”€â”€ checkpoint_info.json    # å…ƒæ•°æ®ï¼ˆå±‚é…ç½®ã€åˆ†æ•°ç­‰ï¼‰
â””â”€â”€ README.txt              # ä½¿ç”¨è¯´æ˜

================================================================================
é‡è¦æç¤º
================================================================================

âš ï¸  Checkpointæ–‡ä»¶å¾ˆå¤§ï¼ˆ~16GBï¼‰ï¼Œä¸è¦æäº¤åˆ°Git
    .gitignoreå·²é…ç½®æ’é™¤model_checkpoints/

âš ï¸  éœ€è¦å…ˆæå–Llamaå±‚æ–‡ä»¶
    è¿è¡Œ: python extract_layers.py --model_name llama --output_dir ../extracted_llama_layers

âš ï¸  åˆ›å»ºcheckpointéœ€è¦GPUæ˜¾å­˜ï¼ˆ~20GBï¼‰
    å¦‚æœGPUä¸è¶³ï¼Œä½¿ç”¨æ˜¾å­˜æ›´å¤§çš„GPUæˆ–å…³é—­å…¶ä»–è¿›ç¨‹

âœ… Checkpointå¯ä»¥è·¨æœºå™¨ä½¿ç”¨
   åªéœ€å¤åˆ¶æ•´ä¸ªcheckpointç›®å½•å³å¯

âœ… CheckpointåŒ…å«å®Œæ•´æ¨¡å‹ï¼Œæ— éœ€åŸå§‹æ¨¡å‹æ–‡ä»¶
   å¯ä»¥ç‹¬ç«‹éƒ¨ç½²å’Œä½¿ç”¨

================================================================================
æ•…éšœæ’é™¤
================================================================================

Q: FileNotFoundError: Layer file not found
A: å…ˆè¿è¡Œ extract_layers.py æå–Llamaå±‚

Q: CUDA out of memory
A: ä½¿ç”¨æ˜¾å­˜æ›´å¤§çš„GPUï¼Œæˆ–æŒ‡å®š--gpuå‚æ•°ä½¿ç”¨å…¶ä»–GPU

Q: åŠ è½½checkpointå¤±è´¥
A: æ£€æŸ¥PyTorchç‰ˆæœ¬ï¼Œç¡®ä¿checkpointæ–‡ä»¶å®Œæ•´

================================================================================
è¯¦ç»†æ–‡æ¡£
================================================================================

ğŸ“š å®Œæ•´æŒ‡å—: MODEL_CHECKPOINTS_GUIDE.md
ğŸ“š é¡¹ç›®ä¸»æ–‡æ¡£: README.md
ğŸ“š å®‰è£…æŒ‡å—: SETUP.md

================================================================================

å¿«é€Ÿå‘½ä»¤é€ŸæŸ¥:

# åˆ›å»ºå•ä¸ªcheckpoint
cd model_preparation && python create_replaced_model_checkpoint.py --layers 11 13 17 21 --output_dir ../model_checkpoints/best

# æ‰¹é‡åˆ›å»º
cd model_preparation && ./create_best_checkpoints.sh

# æµ‹è¯•
cd model_preparation && python test_checkpoint.py --checkpoint ../model_checkpoints/best --full_eval --limit 100

# ä½¿ç”¨
python -c "import torch; model = torch.load('model_checkpoints/best/model.pt'); print('âœ… Loaded!')"

================================================================================
